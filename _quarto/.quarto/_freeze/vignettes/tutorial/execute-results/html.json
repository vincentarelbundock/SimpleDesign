{
  "hash": "363e08dbfce859583c2c9d50a8a6d8b1",
  "result": {
    "engine": "knitr",
    "markdown": "# `SimpleDesign` tutorial\n\n\n\n\n\n\n\n\n\n## Simple data generating process\n\nFirst, we load the package and define a `dgp()` function that encodes a simple data generating process (DGP): A two-arm randomized controlled trial with $N=100$ observations, an outcome $Y$, a random treatment $T$, and a treatment effect of $\\theta=0.5$. \n\nThe `dgp()` function must return a data frame when called. That data frame must also have a \"truth\" attribute, which holds the true value of the estimand that we are targetting.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SimpleDesign)\n\ndgp = function(theta = 0.5, N = 50) {\n  T = rbinom(N, 1, .5) # treatment\n  e = rnorm(N) # noise\n  Y = theta * T + e # outcome\n\n  # output data frame\n  data = data.frame(Y, T)\n\n  # set the \"truth\" attribute\n  attr(data, \"truth\") = theta\n  return(data)\n}\n\ndgp() |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Y T\n1 -0.37302813 0\n2 -1.14314590 1\n3  0.63552187 0\n4 -1.15922120 1\n5 -0.34179292 1\n6 -0.04156007 0\n```\n\n\n:::\n:::\n\n\n\n\nNext, we define a `fit()` function. This function accepts a data frame, fits a model, and returns a data frame of estimates. That data frame must absolutely include an `estimate` column with numeric values. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = function(data) {\n  model = lm(Y ~ T, data = data)\n  results = data.frame(\n    estimator = \"OLS\",\n    estimate = coef(model)[\"T\"]\n  )\n  return(results)\n}\n\ndgp() |> fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimator  estimate\nT       OLS 0.5859094\n```\n\n\n:::\n:::\n\n\n\n\nFinally, we feed both the `dgp()` and the `fit()` functions to the `diagnose()` function. This function will simulate data from the `dgp()` function, fit models using the `fit()` function, and return a data frame with diagnostic statistics. The `truth` attribute of the generated data specifies the true value of the quantity of interest, against which we benchmark estimates.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagnose(dgp, fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimator truth       bias      rmse\n1       OLS   0.5 0.01775981 0.2569797\n```\n\n\n:::\n:::\n\n\n\n\nThe `fit()` function can also produce data frames with additional columns like `conf.low`, `conf.high`, and `p.values`. When those columns are present, `diagnose()` will generate more useful diagnostic statistics.\n\nExtracting that information from models is relatively easy, but it is inconvenient. To make this process easier, `SimpleDesign` supplies the `tidy_estimator()` function. The mandatory `label` argument adds a new column with a unique label. The optional `term` argument specifies the subset of parameters to extract from the model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = function(data) {\n  model = lm(Y ~ T, data = data)\n  results = tidy_estimator(model, label = \"OLS\", term = \"T\")\n  return(results)\n}\n\ndgp() |> fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimator term  estimate   p.value   conf.low conf.high\n2       OLS    T 0.5012431 0.1422529 -0.1682288  1.170715\n```\n\n\n:::\n:::\n\n\n\n\nSince the `fit()` output now includes the $p$ value and confidence interval, `diagnose()` now reports more useful statistics.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagnose(dgp, fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimator term truth         bias      rmse power coverage\n1       OLS    T   0.5 -0.001086504 0.2776061  0.37     0.92\n```\n\n\n:::\n:::\n\n\n\n\n## DGP parameters\n\nThe `dgp()` function has two arguments to control the sample and effect sizes. We can diagnose several research designs in one go by supplying a data frame of DGP parameters to the `dgp_parameters` argument of `diagnose()`. In this example, we use the `expand.grid()` function from base `R` to build a data frame with all combinations of parameter values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparam = expand.grid(N = c(100, 500), theta = c(0.1, 0.5, 1))\nparam\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    N theta\n1 100   0.1\n2 500   0.1\n3 100   0.5\n4 500   0.5\n5 100   1.0\n6 500   1.0\n```\n\n\n:::\n\n```{.r .cell-code}\ndiagnose(dgp, fit, N = 100, dgp_parameters = param)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimator term truth   N theta         bias       rmse power coverage\n1       OLS    T   0.1 100   0.1 -0.000039723 0.17967811  0.07     0.99\n2       OLS    T   0.1 500   0.1  0.007538356 0.09246109  0.23     0.92\n3       OLS    T   0.5 100   0.5  0.052107175 0.20274188  0.84     0.94\n4       OLS    T   0.5 500   0.5  0.005333471 0.08670184  1.00     0.96\n5       OLS    T   1.0 100   1.0  0.010563482 0.21025858  1.00     0.95\n6       OLS    T   1.0 500   1.0  0.003589011 0.08514632  1.00     0.96\n```\n\n\n:::\n:::\n\n\n\n\n## Complex data generating process\n\nSince `dgp()` is just a standard `R` function, users are free to define complex data generating processes using whatever helper functions they wish. For example, the `fabricatr` and `randomizr` packages offer extremely powerful functions to generate simulated data with special random assignment schemes. For example, this `dgp()` generates data from a block random assignment design.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndgp = function(n_blocks = 3, n_indiv = 100, e_sd = 1) {\n  data = fabricatr::fabricate(\n    # block-level variables\n    block = fabricatr::add_level(\n      N = n_blocks,\n\n      # individual treatment effect\n      tau = c(4, 2, 0)\n    ),\n\n    # individual-level variables\n    indiv = fabricatr::add_level(\n      N = n_indiv,\n\n      # noise\n      e = rnorm(N, sd = e_sd),\n\n      # potential outcomes\n      Y_T_0 = e,\n      Y_T_1 = e + tau\n    )\n  )\n  data$T = randomizr::block_ra(blocks = data$block, block_prob = c(.5, .7, .9))\n  data$Y = ifelse(data$T == 1, data$Y_T_1, data$Y_T_0)\n\n  # define truth in terms of potential outcomes\n  attr(data, \"truth\") = mean(data$Y_T_1 - data$Y_T_0)\n\n  return(data)\n}\n\ndgp() |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  block tau indiv          e      Y_T_0    Y_T_1 T          Y\n1     1   4   001  1.0472749  1.0472749 5.047275 1  5.0472749\n2     1   4   002 -0.4902355 -0.4902355 3.509765 0 -0.4902355\n3     1   4   003  2.1545182  2.1545182 6.154518 0  2.1545182\n4     1   4   004  0.9860548  0.9860548 4.986055 0  0.9860548\n5     1   4   005  1.4483193  1.4483193 5.448319 0  1.4483193\n6     1   4   006  0.7895540  0.7895540 4.789554 1  4.7895540\n```\n\n\n:::\n\n```{.r .cell-code}\ndgp() |> attr(\"truth\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n\n\nNow, we define a fit function to see if a \"naive\" linear regression model retrieves a good estimate of the estimand.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = function(data) {\n  model = lm(Y ~ T, data = data)\n  results = tidy_estimator(model, label = \"Naive LM\", term = \"T\")\n  return(results)\n}\n```\n:::\n\n\n\n\nFinally, we diagnose the research design.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagnose(dgp, fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimator term truth      bias      rmse power coverage\n1  Naive LM    T     2 -0.386235 0.4063862     1     0.62\n```\n\n\n:::\n:::\n\n\n\n\n## Robust standard errors\n\nThe confidence interval coverage seems very bad. Could it be because we used classical standard errors when we should have used robust standard errors? To check this possibility, we define a new `fit()` function with a different `vcov` argument in `tidy_estimator()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = function(data) {\n  model = lm(Y ~ T, data = data)\n  results = tidy_estimator(model, label = \"Naive LM\", term = \"T\", vcov = \"HC3\")\n  return(results)\n}\ndiagnose(dgp, fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimator term truth       bias      rmse power coverage\n1  Naive LM    T     2 -0.3949174 0.4138856     1      0.3\n```\n\n\n:::\n:::\n\n\n\n\nNope, the coverage is still awful.\n\n\n## Comparing estimators\n\nPerhaps we should switch estimators. As described on the [DeclareDesign blog](https://declaredesign.org/blog/posts/biased-fixed-effects.html), we could try to control for blocks fixed effects in the linear model, or estimate differences-in-means in each group and then average them. That last option is implemented by the `differences_in_means()` function from the `estimatr` package.\n\nInstead of using the `tidy_estimator()` helper function, we use `tidy_estimator_list()`. This function accepts a named list of models, and returns a simple data frame with appropriate labels. To illustrate, let's simulate a single dataset, store three fitted models in a named list, and call `tidy_estimator_list()`. For fun, we also set the $\\alpha$ level used to build confidence intervals to 0.01.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = dgp()\nresults = list(\n  \"Naive LM\" = lm(Y ~ T, data = data),\n  \"Block controls\" = lm(Y ~ T + block, data = data),\n  \"DinM\" = estimatr::difference_in_means(Y ~ T, blocks = block, data = data)\n)\ntidy_estimator_list(results, term = \"T\", alpha = 0.01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       estimator term estimate      p.value conf.low conf.high\n1       Naive LM    T 1.545227 8.525041e-14 1.011822  2.078631\n2 Block controls    T 2.483890 1.048603e-55 2.076971  2.890809\n3           DinM    T 1.939833 4.032607e-38 1.552748  2.326919\n```\n\n\n:::\n:::\n\n\n\n\nUsing this helper function, we can define a new `fit()` and compare different modelling strategies:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = function(data) {\n  model = lm(Y ~ T, data = data)\n  results = list(\n    \"Naive LM\" = lm(Y ~ T, data = data),\n    \"Block controls\" = lm(Y ~ T + block, data = data),\n    \"DinM\" = estimatr::difference_in_means(Y ~ T, blocks = block, data = data)\n  )\n  results = tidy_estimator_list(results, term = \"T\", alpha = 0.01)\n  return(results)\n}\ndiagnose(dgp, fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       estimator term truth        bias      rmse power coverage\n1 Block controls    T     2  0.57373725 0.5867522     1     0.12\n2           DinM    T     2 -0.00969105 0.1348343     1     1.00\n3       Naive LM    T     2 -0.39144633 0.4080077     1     0.90\n```\n\n\n:::\n:::\n\n\n\n\nThese results show that the difference-in-means strategy yields unbiased results with adequate coverage. We could extend our investigation to consider different DGP parameters:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll}\nparam = expand.grid(n_indiv = c(100, 500), e_sd = c(1, 2))\n\ndiagnose(dgp, fit, dgp_parameters = param)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        estimator term truth n_indiv e_sd         bias       rmse power coverage\n1  Block controls    T     2     100    1  0.610320541 0.62668619     1     0.07\n2  Block controls    T     2     100    2  0.622662622 0.67814423     1     0.65\n3  Block controls    T     2     500    1  0.573770754 0.57712867     1     0.00\n4  Block controls    T     2     500    2  0.605580676 0.61889228     1     0.02\n5            DinM    T     2     100    1  0.022966149 0.15867185     1     0.99\n6            DinM    T     2     100    2  0.037620062 0.29034670     1     0.98\n7            DinM    T     2     500    1 -0.005377531 0.07153573     1     0.98\n8            DinM    T     2     500    2  0.021910332 0.14036368     1     0.97\n9        Naive LM    T     2     100    1 -0.363424865 0.38553073     1     0.95\n10       Naive LM    T     2     100    2 -0.348514140 0.43758231     1     0.95\n11       Naive LM    T     2     500    1 -0.389353547 0.39355522     1     0.00\n12       Naive LM    T     2     500    2 -0.367407353 0.38722720     1     0.48\n```\n\n\n:::\n:::\n",
    "supporting": [
      "tutorial_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}